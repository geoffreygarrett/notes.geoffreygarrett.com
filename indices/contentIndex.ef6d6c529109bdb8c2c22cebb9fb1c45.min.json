{"/":{"title":"Geoffrey's Notes","content":"\n## Maps of Content üó∫Ô∏è\n\nHere are the current maps to my notes. What you see here will change overtime.\nThey won't disappear, they might just be reshuffled and organised under new maps\nthat I deem necessary to add at a later time. For example, I currently have the\nmaps of [software engineering](notes/software-engineering.md) and [machine\nlearning](notes/machine-learning.md) present, with some [machine learning\nalgorithms](notes/dbscan.md) in [software\nengineering](notes/software-engineering.md), both of which _might_ later be\nfound under **computer science**. Feel free to peruse, some notes may have\ncomments enabled if I feel that a note is well-developed enough.\n\n- [Linear Algebra MOC](notes/linear-algebra.md)\n- [Estimation Theory MOC](notes/estimation-theory.md)\n- [Information Theory MOC](notes/information-theory.md)\n- [Software Engineering MOC](notes/software-engineering.md)\n- [Machine Learning MOC](notes/machine-learning.md)\n\n## Useful üõ†Ô∏è\n\nThis is a general catch-all for anything I feel I want to keep visible on the\nsurface. They might help me organise my thoughts in the long-term, or lookup\ninformation and resources that I need I want at my fingertips.\n\n- [Concept Definitions](/notes/concept-definitions.md)\n- [All Notes](/notes)\n\n## Types of Notes ‚ÑπÔ∏è\n\nAs I pursue tending to my digital garden, with new additions, or modifications\nto existing parts, you can expect me to (loosely) follow a (hopefully)\nconsistent categorisation described by the following:\n\n- üå∞ A **Seed** might just be a note, link or some thought. Perhaps a\n  placeholder for something I'd like to pursue later. These may be linked or\n  unlinked, perhaps hidden away where no one sees them.\n- üå± A **Seedling** is more established and ready to continue growing. It has \n  some weak parts, but it's getting there with time. Ideally it won't stay \n  a seedling for much longer, it just needs a bit of tending, water and sunshine.\n- üå≤ **Evergreen** has finished growing. It could do with some pruning once in a \n  while throughout seasons, but it's mature and isn't expected to disappear \n  in our lifetime.\n","lastmodified":"2022-06-22T09:44:18.854296381Z","tags":null},"/notes/cholesky-decomposition":{"title":"Cholesky Decomposition","content":"\nCholesky Decomposition (_a.k.a. Cholesky factorisation_) is a method of \ndecomposing a _positive semidefinite matrix_ into a lower triangular matrix.","lastmodified":"2022-06-22T09:44:18.854296381Z","tags":null},"/notes/concept-definitions":{"title":"Concept Definitions","content":"\nA dynamic collection of concept definitions which are either relevant to my work\nor are of interest to me. These definitions may be referred to from other\nparts of my notes. \n\n## _\"A priori\"_ and _\"a posteriori\"_\n\n\u003e _‚ÄúA priori‚Äù and ‚Äúa posteriori‚Äù refer primarily to how, or on what basis, a\nproposition might be known. In general terms, a proposition is knowable a priori\nif it is knowable independently of experience, while a proposition knowable a\nposteriori is knowable on the basis of experience. The distinction between a\npriori and a posteriori knowledge thus broadly corresponds to the distinction\nbetween empirical and nonempirical knowledge._ - [Internet Encyclopedia of\nPhilosophy](https://iep.utm.edu/apriori/)\n\n- [Further reading](https://en.wikipedia.org/wiki/A_priori_and_a_posteriori)","lastmodified":"2022-06-22T09:44:18.854296381Z","tags":null},"/notes/css-notes":{"title":"Software Engineering","content":"\n- [min(), max(), and clamp(): three logical CSS functions to use today](https://web.dev/min-max-clamp/)","lastmodified":"2022-06-22T09:44:18.854296381Z","tags":null},"/notes/data-sampling-methods":{"title":"Data Sampling Methods","content":"\n## Stochastic Methods\n\n## Deterministic Methods\n\n### Low discrepancy\n\n### Experimental design methods\n\n## Geometric Methods\n\n### Uniform grid\n\n### Latin hypercube\n\n### Centroidal Voronoi Tesellation\n\n### Opposite methods\n\n## Hybrid methods\n\n## Sampling non-uniform distributions\n\n## Importance sampling\n\n## Bibliography\n\n{{\u003c rawhtml \u003e}}\n \u003cul\u003e\n  \u003cli\u003e\n\u003ca href=\"https://www.sciencedirect.com/science/article/pii/S0893608015001768\"\u003e\n\u003cdiv class=\"csl-entry\"\u003e Smart sampling and incremental function learning for\nvery large high dimensional data. (2016). \u003ci\u003eNeural Networks\u003c/i\u003e, \u003ci\u003e78\u003c/i\u003e,\n75‚Äì87. \u003c/div\u003e \u003c/a\u003e\n  \u003c/li\u003e\n\u003c/ul\u003e\n{{\u003c /rawhtml \u003e}}\n\n- [How to evenly distribute points on a sphere more effectively than the canonical Fibonacci Lattice](http://extremelearning.com.au/how-to-evenly-distribute-points-on-a-sphere-more-effectively-than-the-canonical-fibonacci-lattice/)\n- [How to generate uniformly random points on n-spheres and in n-balls](http://extremelearning.com.au/how-to-generate-uniformly-random-points-on-n-spheres-and-n-balls/)","lastmodified":"2022-06-22T09:44:18.854296381Z","tags":null},"/notes/dbscan":{"title":"DBSCAN","content":"\nDensity-based spacial clustering of applications with noise (DBSCAN) is a\n[clustering algorithm](https://en.wikipedia.org/wiki/Cluster_analysis) proposed\nby Ester et al. in 1996. DBSCAN is one of the [most cited clustering\nalgorithms](https://web.archive.org/web/20100421170848/http://academic.research.microsoft.com/CSDirectory/paper_category_7.htm)\nin scientific literature.\n\n```bibtex {linenos=false}\n@inproceedings{Ester1996,\n   author = {Martin Ester and Hans-Peter Kriegel and J√∂rg Sander and Xiaowei Xu},\n   journal = {KDD},\n   title = {A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise},\n   year = {1996},\n}\n```\n\n## Resources\n\n- [How HDBSCAN Works](https://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html)","lastmodified":"2022-06-22T09:44:18.854296381Z","tags":null},"/notes/deep-learning":{"title":"Deep Learning","content":"\nDeep Learning (DL) is a field of Machine Learning (ML) that is primarily\nconcerned with the learning of representations of data. At the core of DL is the\nuse of Multi-layer perceptrons (MLPs), used to model these representations. MLPs\nare fully connected layers of biologically inspired artifical neurons, also\nknown as perceptrons. A brief history are these biologically inspired models are\ncovered in Section X with adapted notation from the field of DL. Although not\nall practices in DL, strictly speaking, make use of MLPs, they are a fundamental\nconcept which must be understood in the stepping stones towards concepts of\nhigher complexity in the field. Section X covers this concept, extending\ndirectly on their composite component: perceptrons.\n\n**MLP** are also called **feedfoward** as information is propagated in\nonly a forward direction, as opposed to exhibiting **feedback** connections,\nwhere intermediate computations are fed back into the network. When feedforward\nnetworks are extended to include feedback connections, they are called\n**RNNs**. These types of networks excel at learning temporal\nfeatures, exhibiting a refined hypothesis space favouring sequenced information,\nsuch as a series of chronological observations. Section X covers this\ntype of DNN, and the prominent sub-type of RNN: LSTMs.\n\nOne type of neural networks which is similar to MLP and popular in\ncontemporary research, is the CNN. These deep NN are essentially\nMLP which omit the property of being fully connected, in favour of\nrefining the hypothesis space towards detection of spatially-related features.","lastmodified":"2022-06-22T09:44:18.854296381Z","tags":null},"/notes/estimation-theory":{"title":"Estimation Theory","content":"\n**Estimation theory** is a branch of statistics that addresses the estimation of\nunknown parameters based on empirical measurements, which contain a random\ncomponent.\n\n- An **estimator** is an algorithm that attempts to approximate the unknown\n  parameters using measurements.\n\n## Estimators \n\n- [Least squares](notes/least-squares.md)\n- [Kalman filter](notes/kalman-filter.md)\n\n## Applications\n\n- [Orbit determination](notes/orbit-determination.md)\n","lastmodified":"2022-06-22T09:44:18.854296381Z","tags":null},"/notes/information-theory":{"title":"Information Theory","content":"\n## Self-information\n\nThe self-information (a.k.a. _information content_, _surprisal_,\nor _Shannon information_) is a quantity used in information theory which\nis derived from the probability of a certain event occurring from a random\nvariable. The self-information was defined by Claude Shannon such that\nthe following axioms were met:\n\n- An event that is 100% probable is perfectly unsurprising and\n  therefore yields no information content.\n- The less probable an event is, the more surprising, and therefore the\n  more information it yields.\n- The total information of independently measured events, is the\n  sum of their respective self-information.\n\n$$\n\\begin{equation}\n    I(x):=-\\log_b(p(x))\n\\end{equation}\n$$\n\n## Entropy\n\n$$\n\\begin{equation}\n    H(x)=\\mathbb{E}[I(x)]\n\\end{equation}\n$$\n\n## Kullback-Leibler divergence\nThe Kullback-Leibler divergence (a.k.a. _information gain_, _relative entropy_\nand _I-divergence_) is a measure of how one probability distribution $P$ differs\nfrom another, $Q$. For the distributions $P$ and $Q$ for a continuous random\nvariable, the relative entropy integral is:\n\n$$\n\\begin{equation}\n    D_{KL}(P||Q) = \\int_{-\\infty}^{\\infty}p(x)\\log\\bigg(\\frac{p(x)}{q(x)}\\bigg)dx.\n\\end{equation}\n$$\n\nNote that $D_{KL}(P||Q)$ is only finite if the support set of $P$ is\ncontained in the support set of $Q$. In the context of Bayesian inference\n$D_{KL}(P||Q)$, read as _the KL-divergence of P given Q_, is a\nmeasure of the information gained by revisiting one's beliefs from a prior\nprobability distribution $Q$ to a posterior probability distribution $P$. For\nexample the $D_{KL}(P||Q)$ for $P\\sim{}\\mathcal{N}(\\mu_1,\\sigma_1)$ and\n$Q\\sim{}\\mathcal{N}(\\mu_2,\\sigma_2)$ can be derived analytically to be:\n\n$$\n\\begin{equation}\n    D_{KL}(P||Q) = \\log\\frac{\\sigma_2}{\\sigma_1} + \\frac{\\sigma_1^2 + (\\mu_1-\\mu_2)^2}{2\\sigma_2^2} - \\frac{1}{2}.\n\\end{equation}\n$$\n\nOne noteworthy characteristic of the KL-divergence is its asymmetry, that is\n$D_{KL}(P||Q)\\neq{}D_{KL}(Q||P)$. This means that KL-divergence makes\nfor a poor _distance_ metric as is commonly done with, for example,\nsquared-errors. This may at first present KL-divergence as a suboptimal choice\nas a general metric, however when considering the relation between the posterior\nand the priori, the relative information gain when travelling from one to the\nother **is** inherently asymmetric by their very nature. \n\n## Jensen-Shannon divergence\n\nThe Jensen-Shannon divergence (a.k.a. _information radius_ and _total divergence\nto the average_) is based on the KL-divergence, however it has been extended\nwith the differences that it is symmetric, and always has a finite value.\n\n$$\n\\begin{equation}\n    D_{JS}(P||Q) = \\frac{1}{2}D_{KL}(P||M) + \\frac{1}{2}D_{KL}(Q||M)\n\\end{equation}\n$$\n$$\n\\text{where  }\nM = \\frac{1}{2}(P+Q)\n$$","lastmodified":"2022-06-22T09:44:18.886296809Z","tags":null},"/notes/kalman-filter":{"title":"Kalman Filter","content":"\n## Linearised Kalman filter\n\n## Extended Kalman filter\n\n- Non-linear filter\n- [Hidden Markov model](notes/markov-models.md#hidden-markov-model)\n\n## Unscented Kalman filter\n\n- Non-linear filter\n\n$$\n\\begin{equation}\n    \\begin{aligned}\n        L                   \u0026= \\text{dim}(\\hat{\\bm{x}})                                                      \u0026\u0026 \\bm{x}\\in{\\mathbb{R}}^{L} \\\\\n        \\mathcal{X}_0       \u0026= \\hat{\\bm{x}} _{i-1}^{+}                                                                                     \\\\\n        \\mathcal{X}_j       \u0026= \\hat{\\bm{x}} _{i-1}^{+} + \\sqrt{(L+\\lambda)} \\bm{A}_j                         \u0026\u0026 j=1,...,L                 \\\\\n        \\mathcal{X} _{L+j}  \u0026= \\hat{\\bm{x}} _{i-1}^{+} - \\sqrt{(L+\\lambda)} \\bm{A}_j                         \u0026\u0026 j=1,...,L                 \\\\\n        W_0^{(m)}           \u0026= \\lambda(L-\\lambda)                                                                                         \\\\\n        W_0^{(c)}           \u0026= \\lambda(L-\\lambda) + (1-\\alpha^2 + \\beta)                                                                  \\\\\n        W_k^{(m)}           \u0026= W_k^{(c)} = 1/\\{2(L+\\lambda)\\}                                                \u0026\u0026 j=1,...,2L                \\\\\n    \\end{aligned}\n\\end{equation}\n$$\n\n$$\n\\begin{aligned}\n\\text{where  }\n    \\lambda \u0026= \\alpha^2(L+\\kappa)-L,\\text{ a scaling parameter,} \\\\\n    \\alpha  \u0026= \\text{parameter determining spread of sigma points about }\\hat{\\bm{x}}\\text{,} \\\\\n    \\kappa  \u0026= \\text{secondary scaling parameter, usually set to }0, \\\\\n    \\beta   \u0026= \\text{ parameter for incorporation of prior knowledge of }p(\\hat{\\bm{x}}).\\\\\n\\end{aligned}\n$$\n\n{{\u003c svg src=\"/notes/images/sigma-points-wan.svg\" caption=\"Visual representation of sigma points, $\\mathcal{X}_j$, using Wan and van der Merwe's proposed parametrisation. $\\kappa=0$, $\\alpha=1$\" \u003e}}\n\n[comment]: \u003c\u003e ({{\u003c figure src=\"/notes/images/sigma-points-wan.svg\" caption=\"Visual representation of sigma points, $\\mathcal{X}_j$, using Wan and van der Merwe's proposed method.\" \u003e}})\n\n\n## Discriminative Kalman filter\n\n- Non-linear filter\n\n## Inspiration for distillation\n\n- [How a Kalman filter works, in pictures](http://www.bzarg.com/p/how-a-kalman-filter-works-in-pictures/)\n\n","lastmodified":"2022-06-22T09:44:18.886296809Z","tags":null},"/notes/least-squares":{"title":"Least Squares","content":"\nThe least-squares method, a method of solving overdetermined sets of linear\nequations, was officially discovered and published by Adrien-Marie Legendre in\ntheir work _\"Nouvelles methodes pour la determination des orbites des cometes\"_,\npublished in 1805.\n\n```bibtex {linenos=false}\n@Book{Legendre1805,\n    author = {Legendre, A. M.},\n    title = {Nouvelles methodes pour la determination des orbites des cometes [microform] / par A.M. Legendre},\n    publisher = {F. Didot Paris},\n    pages = {viii, 80 p., [1] leaf of plates :},\n    year = {1805},\n    type = {Book, Microform},\n    language = {French},\n    subjects = {Comets -- Orbits.},\n    life-dates = {1970 - 1805},\n    catalogue-url = {https://nla.gov.au/nla.cat-vn866184},\n}\n```\n\n## Ordinary Least Squares\n\nOrdinary Least Squares (OLS)\n\n$$\n\\begin{equation}\n    {\\hat {\\beta }}={\\rm {arg}}\\min _{\\beta }\\,\\lVert z-X\\beta \\rVert,\n\\end{equation}\n$$\n\n- [[Linear Algebra|test]]\n- [[Linear Algebra]]\n\n- **Linearity in parameters**: The system model is _linear in\n  parameters_, that is, [[Linear Algebra#(System of) Linear Equation(s)|$\\bm{z} = \\bm{A}\\bm{\\beta}+\\bm{\\epsilon}$]].\n- **Strict exogenity**: The errors in the regression are should have conditional mean\n  zero, that is, $\\mathbb{E}[\\bm{\\epsilon}|\\bm{A}] = \\bm{0}$.\n\n{{\u003c svg src=\"/notes/images/OLS-geometric-interpretation.svg\" caption=\"Geometric interpretation of Ordinary Least Squares (OLS).\" \u003e}}\n\n## Weighted Least Squares\n\nThe Weighted Least Squares (WLS) method is an application of the\nGeneralised Least Squares (GLS) algorithm, which aims at\nestimating unknown parameters ($\\bm{\\beta}$) in a linear regression model, given a\nset of observations ($\\bm{z}$), where there is a certain degree of correlation\n($\\bm{W}$) between the residuals ($\\bm{\\epsilon}$) in the regression model. It\nis usually written as:\n\n$$\n\\begin{equation}\n    \\begin{aligned}\n        \\bm{z} \u0026= \\bm{A}\\bm{\\beta}+\\bm{\\epsilon}, \\\\\n        \\mathbb{E}[\\bm{\\epsilon}|\\bm{A}] \u0026= \\bm{0}, \\\\\n        \\text{Cov}(\\bm{\\epsilon}|\\bm{A}) \u0026= \\bm{W}. \\\\\n    \\end{aligned}\n\\end{equation}\n$$\n\nThe residual vector is defined as $\\rho=\\bm{z}-\\bm{A}\\bm{\\beta}$. The Weighted\nLeast Squares estimate ($\\bm{\\hat{\\beta}}$) is unbiased, consistent and\nefficient, and obtained through minimising $\\bm{\\rho}^T\\bm{W}^{-1}\\bm{\\rho}$. The\nestimate is given, without derivation, by:\n\n$$\n\\begin{equation}\n    \\begin{aligned}\n        \\bm{\\hat{\\beta}} \u0026= (\\bm{A}^T\\bm{W}^{-1}\\bm{A})^{-1}\\bm{A}^T\\bm{W}^{-1}\\bm{z}, \\\\\n        \\mathbb{E}[\\bm{\\hat{\\beta}}] \u0026= \\bm{\\beta}, \\\\\n        \\text{Cov}[\\bm{\\hat{\\beta}}|\\bm{A}] \u0026= (\\bm{A}^T\\bm{W}^{-1}\\bm{A})^{-1}. \\\\\n    \\end{aligned}\n\\end{equation}\n$$\n\n## Generalised Least Squares\n\nGLS was first described by Alexander Aitken in 1936. \\[[1](https://en.wikipedia.org/wiki/Generalized_least_squares#cite_note-1)\\]\n\n## Non-linear Least Squares\n\nThe problem arises when considering the highly non-linear modelled measurements\n$\\bm{h}(\\bm{x}_0)$ cannot be described by the linear relation above. For this\nreason, the model is linearized by approximation to a first-order Taylor\npolynomial expansion about $\\bm{x}_0^k$, where $k$ is an iteration number:\n\n$$\n\\begin{equation}\n    \\begin{aligned}\n        \\bm{z}                              \u0026=        \\bm{f}(\\bm{\\beta}) + \\bm{\\epsilon}                                                              \\\\\n        \\bm{f}(\\bm{\\beta}+\\Delta\\bm{\\beta}) \u0026\\approx  \\bm{f}(\\bm{\\beta}) + \\frac{\\partial{\\bm{f}(\\bm{\\beta})}}{\\partial{\\bm{\\beta}}}\\Delta\\bm{\\beta}  \\\\\n    \\end{aligned}\n\\end{equation}\n$$\n","lastmodified":"2022-06-22T09:44:18.886296809Z","tags":null},"/notes/linear-algebra":{"title":"Linear Algebra","content":"\n## Definitions\n\n### (System of) Linear Equation(s)\n\nA system of linear equations may be represented as the matrix equation:\n\n$$\n\\bm{A}\\bm{x} = \\bm{b},\n$$\n\nand is equivalent to the following systems of linear equations:\n\n$$\n\\begin{aligned} \n    a_{1,1}x_{1} + a_{1,2}x_{2} + \u0026\\cdots + a_{1,n}x_{n} = b_{1} \\\\\n                                  \u0026\\ \\ \\vdots                    \\\\\n    a_{m,1}x_{1} + a_{m,2}x_{2} + \u0026\\cdots + a_{m,n}x_{n} = b_{m} \n\\end{aligned}\n$$\n\n## Matrix Decomposition\n\n- [Cholesky Decomposition](notes/cholesky-decomposition.md)\n- [Singular Value Decomposition](notes/singular-value-decomposition.md)\n\n## Definitions\n\n- A symmetric matrix $\\bm{A}\\n\\mathbb{R}^{n\\times{n}}$ is positive semidefinite if:\n\n$$\n\\bm{x}^T\\bm{A}\\bm{x}\\geq 0 \\forall\\,\\bm{x} \\in\\mathbb{R}^n.\n$$\n\n- A symmetric matrix $\\bm{A}\\n\\mathbb{R}^{n\\times{n}}$ is positive definite if:\n\n$$\n\\bm{x}^T\\bm{A}\\bm{x}\u003e 0 \\forall\\,\\bm{x}\\neq 0 \\in\\mathbb{R}^n.\n$$","lastmodified":"2022-06-22T09:44:18.886296809Z","tags":null},"/notes/machine-learning":{"title":"Machine Learning","content":"\n## Sub-fields\n- [Deep Learning](notes/deep-learning.md)\n\n## Useful Topics\n- [Data Sampling Methods](notes/data-sampling-methods.md)\n","lastmodified":"2022-06-22T09:44:18.886296809Z","tags":null},"/notes/markov-models":{"title":"Markov Models","content":"\n{{\u003c rawhtml \u003e}}\n\u003ctable class=\"tg\"\u003e\n\u003cthead\u003e\n  \u003ctr\u003e\n    \u003cth class=\"tg-fymr\"\u003eCharacterization\u003c/th\u003e\n    \u003cth class=\"tg-fymr\"\u003eFully Observable\u003c/th\u003e\n    \u003cth class=\"tg-fymr\"\u003ePartially Observable\u003c/th\u003e\n  \u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n  \u003ctr\u003e\n    \u003ctd class=\"tg-fymr\"\u003eAutonomous\u003c/td\u003e\n    \u003ctd class=\"tg-0pky\"\u003eMarkov chain/ process\u003c/td\u003e\n    \u003ctd class=\"tg-0pky\"\u003eHidden Markov model\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd class=\"tg-fymr\"\u003eControlled\u003c/td\u003e\n    \u003ctd class=\"tg-0pky\"\u003eMarkov decision process\u003cbr\u003e\u003c/td\u003e\n    \u003ctd class=\"tg-0pky\"\u003ePartially observable Markov decision process\u003c/td\u003e\n  \u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n{{\u003c /rawhtml \u003e}}\n\n## Hidden Markov model\n\n### Markov chains and processes\n\n- A Markov chain is a discrete-time process.\n- A Markov process is a continuous-time process.\n","lastmodified":"2022-06-22T09:44:18.886296809Z","tags":null},"/notes/notes-website":{"title":"Quartz, Obsidian, and Hugo","content":"\n# Test \n\n```shell\nmkdir %USERPROFILE%/src\ncd %USERPROFILE%/src\ngit clone https://github.com/gohugoio/hugo.git\ncd hugo\ngo install --tags extended -v -x\n```\n","lastmodified":"2022-06-22T09:44:18.886296809Z","tags":null},"/notes/orbit-determination":{"title":"Orbit Determination and Parameter Estimation","content":"\nPredicting the state ($\\bm{x}_t$) of a spacecraft given an initial condition\n($\\bm{x}_0$), and models which form the equations of motion of the satellite,\n($\\dot{\\bm{x}}=f(t,\\bm{x})$), is a straightforward task involving the solution\nof an initial value problem (IVP) in the form of an ordinary differential\nequation (ODE). However the inverse problem  is more involved, that is, given a\nset of measurements ($\\bm{z}$) resulting from the dynamical system, we would\nlike to estimate the trajectory of the satellite and the parameters describing\nthe dynamical models, described mathematically as:\n\n$$\n\\begin{equation}\n    \\bm{x}(t) =\n    \\begin{bmatrix}\n        \\bm{r}(t) \\\\\n        \\bm{v}(t) \\\\\n        \\bm{p} \\\\\n        \\bm{q} \\\\\n    \\end{bmatrix},\n\\end{equation}\n$$\n\n$$\n\\begin{aligned}\n    \\textrm{where  }\n        \\bm{r}(t), \\bm{v}(t) \u0026= \\text{the position and velocity of the spacecraft as a function of time,} \\\\\n        \\bm{p}               \u0026= \\text{the parameters describing the force models,} \\\\\n        \\bm{q}               \u0026= \\text{the parameters describing the measurement models.} \\\\\n\\end{aligned}\n$$\n\nThe measurements made throughout the trajectory of the spacecraft at times\n$t_1,...,t_n$ are described by $\\bm{z}=[z_1,...,z_n]^T$, where each $z_i$ is\neither defined as a function of the state of the spacecraft at time $t_i$, or\nas a function of the state of the spacecraft at time $t_0$:\n\n$$\n\\begin{equation}\n    z_i(t_i) = g_i(t_i, \\bm{x}(t_i))+\\epsilon_i = h_i(t_i, \\bm{x}_0)+\\epsilon_i.\n\\end{equation}\n$$\n\n$$\n\\begin{aligned}\n    \\textrm{where  }\n        z_i \u0026= \\text{the i}^{th}\\text{ empirical measurement, assumed to be a random variable,} \\\\\n        g_i \u0026= \\text{the i}^{th}\\text{ model measurement as a function time and instantaneous state,} \\\\\n        h_i \u0026= \\text{the i}^{th}\\text{ model measurement as a function time and initial state,} \\\\\n        \\epsilon_i \u0026= \\text{the i}^{th}\\text{ residual, accounting for measurement errors.} \\\\\n\\end{aligned}\n$$\n\nThe expressions of $h_i$ and $g_i$ can be used interchangeably in the\nmeasurement model predictions, to account for the fact that the measurements are\noften made at different times than the respective instantaneous states of the\nspacecraft. This is done using variational equations, which are simulated to\nobtain the state transition matrix $\\bm{\\Phi}(t_0, t)$ of the spacecraft, which\nmay be interpolated for any arbitrary time within the temporal bounds of the ODE\nsolution across $t_i\\in[t_0, t_f]$, so that one may\nrelate an empirical $z_i$ at $\\bm{x}(t_i)$ to $\\bm{x}_0$ through $\\bm{\\Phi}(t_0,\nt_i)^{-1}\\bm{x}_0$. This effectively constrains the trajectory to the designed m\nIVP dynamical solution. Consequentially, the measurements concisely:\n\n$$\n\\begin{equation}\n    \\bm{z} = \\bm{h}(\\bm{x}_0) + \\bm{\\epsilon}.\n\\end{equation}\n$$\n","lastmodified":"2022-06-22T09:44:18.886296809Z","tags":null},"/notes/singular-value-decomposition":{"title":"","content":"","lastmodified":"2022-06-22T09:44:18.886296809Z","tags":null},"/notes/software-engineering":{"title":"Software Engineering","content":"## Algorithms\n- [Density-based spacial clustering of applications with noise (DBSCAN)](notes/dbscan.md)\n\n## Object Oriented\n- [SOLID Principles](notes/solid-principles.md)\n\n## Frontend\n- [CSS Tips](notes/css-notes.md)\n- [three.js basics](notes/threejs-basics.md)\n\n## Personalised Development\n- [Windows Development](notes/windows-development.md)","lastmodified":"2022-06-22T09:44:18.886296809Z","tags":null},"/notes/solid-principles":{"title":"SOLID Principles","content":"\nIn software engineering, **SOLID** is a mnemonic acronym for five design\nprinciples intended to make object-oriented designs more **understandable**,\n**flexible**, and **maintainable**.\n\n## **S**ingle responsibility\n\n\u003e There should never be more than one reason for a class to change.\n\n## **O**pen‚Äìclosed\n\n\u003e Software entities ... should be open for extension, but closed for modification.\n\n## **L**iskov substitution\n\n\u003e Functions that use pointers or references to base classes must be able to use objects of derived classes without knowing it.\n\n## **I**nterface segregation\n\n\u003e Many client-specific interfaces are better than one general-purpose interface.\n\n## **D**ependency inversion\n\n\u003e Depend upon abstractions, \\[not\\] concretions.","lastmodified":"2022-06-22T09:44:18.886296809Z","tags":null},"/notes/threejs-basics":{"title":"three.js Library","content":"\n`three.js` is a cross-browser Javascript API\nwhich allows for the creation and display of\n3D computer graphics in a web browser using \n[WebGL](https://en.wikipedia.org/wiki/WebGL?wprov=sfla1)\n(a lower level Javascript API allowing\nfor GPU-accelerated physics and image \nprocessing locally, as part of \nthe web page canvas). There exists a \n[plethora of examples](https://threejs.org/)\nwhich can be used as starting points in your\nprojects, one of which I often come\nsee when\n[signing into GitHub](https://github.com/home).\n\n````html {linenostart=1, linenos=false, title=\"index.html\"}\n\u003c!DOCTYPE html\u003e\n\u003chtml\u003e\n\u003chead\u003e\n    \u003cmeta charset=\"utf-8\"\u003e\n    \u003ctitle\u003eMy first three.js app\u003c/title\u003e\n    \u003cstyle\u003e body {\n        margin: 0;\n    } \u003c/style\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003cscript src=\"js/three.js\"\u003e\u003c/script\u003e\n\u003cscript\u003e\n    // Our Javascript will go here.\n\u003c/script\u003e\n\u003c/body\u003e\n````\n\nThe main components of any `three.js` app are the scene, the camera, and the\nrenderer. These are setup as demonstrated in the following code:\n\n````javascript {title=\"main.js\", linenos=false}\n// 1. Create a scene\nvar scene = new THREE.Scene();\n\n// 2. Create a camera\nvar camera = new THREE.PerspectiveCamera(75, window.innerWidth / window.innerHeight, 0.1, 1000);\n\n// 3. Create a render\nvar renderer = new THREE.WebGLRenderer();\n\n// 4. Set the size of the render\nrenderer.setSize(window.innerWidth, window.innerHeight);\n````\n\n- There are different types of cameras in `three.js`:\n- [PerspectiveCamera](https://threejs.org/docs/#api/cameras/PerspectiveCamera)\n- [OrthographicCamera](https://threejs.org/docs/#api/cameras/OrthographicCamera)\n- [CubeCamera](https://threejs.org/docs/#api/cameras/CubeCamera)\n- [StereoCamera](https://threejs.org/docs/#api/cameras/StereoCamera)\n- [ArrayCamera](https://threejs.org/docs/#api/cameras/ArrayCamera)\n- [Camera](https://threejs.org/docs/#api/cameras/Camera)\n\nBut for now, we'll stick with the PerspectiveCamera.\n\n`PerspectiveCamera(fov, aspect, near, far)`\n\n- `near`: The near clipping plane. Nothing is rendered before this distance.\n- `far`: The far clipping plane. Nothing is rendered after this distance.\n- `fov`: The field of view.\n- `aspect`: The aspect ratio.\n\n**Want to render at a different resolution?** \n\u003e Use the `setSize` method. If you wish to keep the size of your app but render\nit at a lower resolution, you can do so by calling setSize with false as\nupdateStyle (the third argument). For example, `setSize(window.innerWidth/2,\nwindow.innerHeight/2, false)` will render your app at half resolution, given\nthat your `\u003ccanvas\u003e` has 100% width and height.\n\nLet's add a cube to our scene.\n````js {linenos=false}\nconst geometry = new THREE.BoxGeometry(1, 1, 1);\nconst material = new THREE.MeshBasicMaterial({color: 0x00ff00});\nconst cube = new THREE.Mesh(geometry, material);\nscene.add(cube);\ncamera.position.z = 5;\n````\n\n\u003e By default, when we call scene.add(), the thing we add will be added to the\ncoordinates (0,0,0). This would cause both the camera and the cube to be inside\neach other. To avoid this, we simply move the camera out a bit.\n\n**Rendering the scene**\n\n\u003e If you copied the code from above into the HTML file we created earlier, you\nwouldn't be able to see anything. This is because we're not actually rendering\nanything yet. For that, we need what's called a render or animate loop.\n\n### Resources\n\n- [Using Three.js to Add 3D Elements to your Websites](https://www.elegantthemes.com/blog/design/using-three-js-to-add-3d-elements-to-your-websites)","lastmodified":"2022-06-22T09:44:18.886296809Z","tags":null},"/notes/windows-development":{"title":"Windows Development","content":"\n\n- [Setting up Windows Terminal, WSL and Oh-my-Zsh](https://www.ivaylopavlov.com/setting-up-windows-terminal-wsl-and-oh-my-zsh/#.YrHoEVTMKh9)","lastmodified":"2022-06-22T09:44:18.886296809Z","tags":null}}