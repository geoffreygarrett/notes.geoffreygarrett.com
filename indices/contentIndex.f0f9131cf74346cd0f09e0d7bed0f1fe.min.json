{"/":{"title":"Geoffrey's Notes","content":"\n- [Machine Learning MOC](notes/machine-learning.md)\n- [Estimation Theory MOC](notes/estimation-theory.md)\n- [Information Theory MOC](notes/information-theory.md)\n\n- [All Notes](/notes)\n","lastmodified":"2022-05-29T17:21:50.297778023Z","tags":null},"/notes/estimation-theory":{"title":"Estimation Theory","content":"\n**Estimation theory** is a branch of statistics that addresses the estimation of\nunknown parameters basic on empirical measurements, which contain a random\ncomponent.\n\n- An **estimator** is an algorithm that attempts to approximate the unknown\n  parameters using measurements.\n\n## Estimators \n\n- [Least squares](notes/least-squares.md)\n\n## Applications\n\n- [Orbit determination](notes/orbit-determination.md)\n","lastmodified":"2022-05-29T17:21:50.297778023Z","tags":null},"/notes/information-theory":{"title":"Information Theory","content":"\n## Self-information (information content)\n\nThe self-information (a.k.a. _information content_,_surprisal_,\nor _Shannon information_) is a quantity used in information theory which\nis derived from the probability of a certain event occurring from a random\nvariable. The self-information was defined by Claude Shannon such that\nthe following axioms were met:\n\n- An event that is 100% probable is perfectly unsurprising and\n  therefore yields no information content.\n- The less probable an event is, the more surprising, and therefore the\n  more information it yields.\n- The total information of independently measured events, is the\n  sum of their respective self-information.\n\n$$\n\\begin{equation}\n    I(x):=-\\log_b(p(x))\n\\end{equation}\n$$\n\n\\subsection{Entropy (average information)}\n\n$$\n\\begin{equation}\n    H(x)=\\mathbb{E}[I(x)]\n\\end{equation}\n$$\n\n## Kullback-Leibler divergence (information gain)\n\nThe Kullback-Leibler divergence (a.k.a. _relative entropy_ and\n_I-divergence_) a measure of how one probability distribution $P$ differs\nfrom another, $Q$. For the distributions $P$ and $Q$ for a continuous random\nvariable, the relative entropy integral is:\n\n$$\n\\begin{equation}\n    D_{KL}(P\\;||\\;Q) = \\int_{-\\infty}^{\\infty}p(x)\\log(\\frac{p(x)}{q(x)})\\;dx.\n\\end{equation}\n$$\n\nNote that $D_{KL}(P\\;||\\;Q)$ is only finite if the support set of $P$ is\ncontained in the support set of $Q$. In the context of Bayesian inference\n$D_{KL}(P\\;||\\;Q)$, read as _the KL-divergence of P given Q_, is a\nmeasure of the information gained by revisiting one's beliefs from a prior\nprobability distribution $Q$ to a posterior probability distribution $P$. For\nexample the $D_{KL}(P\\;||\\;Q)$ for $P\\sim{}\\mathcal{N}(\\mu_1,\\sigma_1)$ and\n$Q\\sim{}\\mathcal{N}(\\mu_2,\\sigma_2)$ can be derived analytically to be:\n\n$$\n\\begin{equation}\n    D_{KL}(P\\;||\\;Q) = \\log\\frac{\\sigma_2}{\\sigma_1} + \\frac{\\sigma_1^2 + (\\mu_1-\\mu_2)^2}{2\\sigma_2^2} - \\frac{1}{2}.\n\\end{equation}\n$$\n\nOne noteworthy characteristic of the KL-divergence is its asymmetry, that is\n$D_{KL}(P\\;||\\;Q)\\neq{}D_{KL}(Q\\;||\\;P)$. This means that KL-divergence makes\nfor a poor \\textit{distance} metric as is commonly done with, for example,\nsquared-errors. This may at first present KL-divergence as a suboptimal choice\nas a general metric, however when considering the relation between the posterior\nand the priori, the relative information gain when travelling from one to the\nother **is** inherently asymmetric by their very nature. \n\n## Jensen-Shannon divergence (information radius)\n\nThe Jensen-Shannon divergence (a.k.a. _information radius_ and _total divergence\nto the average_) is based on the KL-divergence, however it has been extended\nwith the differences that it is symmetric, and always has a finite value.\n\n$$\n\\begin{equation}\n    D_{JS}(P\\;||\\;Q) = \\frac{1}{2}D_{KL}(P\\;||\\;M) + \\frac{1}{2}D_{KL}(Q\\;||\\;M)\n\\end{equation}\n$$\n$$\n\\text{where}\\;\\;\nM = \\frac{1}{2}(P+Q)\n$$","lastmodified":"2022-05-29T17:21:50.301778025Z","tags":null},"/notes/least-squares":{"title":"Least Squares","content":"\n{{\u003c svg src=\"/notes/images/OLS_geometric_interpretation.svg\" caption=\"Geometric interpretation of Ordinary Least-Squares (OLS).\" \u003e}}\n\n## Weighted Least-Squares Estimation\n\nThe Weighted Least-Squares Estimation method is an application of the\nGeneralized Least-Squares (GLS) algorithm, which aims at\nestimating unknown parameters ($\\bm{\\beta}$) in a linear regression model, given a\nset of observations ($\\bm{z}$), where there is a certain degree of correlation\n($\\bm{W}$) between the residuals ($\\bm{\\epsilon}$) in the regression model. It\nis usually written as:\n\n$$\n\\begin{equation}\n    \\begin{aligned}\n        \\bm{z} \u0026= \\bm{A}\\bm{\\beta}+\\bm{\\epsilon}, \\\\\n        \\mathbb{E}[\\bm{\\epsilon}|\\bm{A}] \u0026= \\bm{0}, \\\\\n        \\text{Cov}(\\bm{\\epsilon}|\\bm{A}) \u0026= \\bm{W}. \\\\\n    \\end{aligned}\n\\end{equation}\n$$\n\nThe residual vector is defined as $\\rho=\\bm{z}-\\bm{A}\\bm{\\beta}$. The Weighted\nLeast-Squares estimate ($\\bm{\\hat{\\beta}}$) is unbiased, consistent and\nefficient, and obtained through minimising $\\bm{\\rho}^T\\bm{W}^{-1}\\bm{\\rho}$. The\nestimate is given, without derivation, by:\n\n$$\n\\begin{equation}\n    \\begin{aligned}\n        \\bm{\\hat{\\beta}} \u0026= (\\bm{A}^T\\bm{W}^{-1}\\bm{A})^{-1}\\bm{A}^T\\bm{W}^{-1}\\bm{z}, \\\\\n        \\bm{\\hat{\\beta}} \u0026= \\text{argmin}(\\bm{\\rho}^T\\bm{W}^{-1}\\bm{\\rho}), \\\\\n        \\mathbb{E}[\\bm{\\hat{\\beta}}] \u0026= \\bm{\\beta}, \\\\\n        \\text{Cov}[\\bm{\\hat{\\beta}}|\\bm{A}] \u0026= (\\bm{A}^T\\bm{W}^{-1}\\bm{A})^{-1}. \\\\\n    \\end{aligned}\n\\end{equation}\n$$\n\n## Non-linear Weighted Least-Squares Estimation\n\nThe problem arises when considering the highly non-linear modelled measurements\n$\\bm{h}(\\bm{x}_0)$ cannot be described by the linear relation above. For this\nreason, the model is linearized by approximation to a first-order Taylor\npolynomial expansion about $\\bm{x}_0^k$, where $k$ is an iteration number:\n\n$$\n\\begin{equation}\n    \\begin{aligned}\n        \\bm{z}                              \u0026=        \\bm{f}(\\bm{\\beta}) + \\bm{\\epsilon}                                                              \\\\\n        \\bm{f}(\\bm{\\beta}+\\Delta\\bm{\\beta}) \u0026\\approx  \\bm{f}(\\bm{\\beta}) + \\frac{\\partial{\\bm{f}(\\bm{\\beta})}}{\\partial{\\bm{\\beta}}}\\Delta\\bm{\\beta}  \\\\\n    \\end{aligned}\n\\end{equation}\n$$\n","lastmodified":"2022-05-29T17:21:50.301778025Z","tags":null},"/notes/machine-learning":{"title":"Deep Learning Introduction","content":"\n## Deep Learning\nDL is a field of ML that is primarily concerned with the learning of\nrepresentations of data. At the core of DL is the use of MLP, used\nto model these representations. MLP are fully connected layers of\nbiologically inspired artifical neurons, also known as perceptrons. A\nbrief history are these biologically inspired models are covered in\nSection X with adapted notation from the field of DL.\nAlthough not all practices in DL, strictly speaking, make use of\nMLPs, they are a fundamental concept which must be understood in the\nstepping stones towards concepts of higher complexity in the field.\nSection X covers this concept, extending directly on their composite\ncomponent: perceptrons.\n\n**MLP** are also called **feedfoward** as information is propagated in\nonly a forward direction, as opposed to exhibiting **feedback** connections,\nwhere intermediate computations are fed back into the network. When feedforward\nnetworks are extended to include feedback connections, they are called\n**RNNs**. These types of networks excel at learning temporal\nfeatures, exhibiting a refined hypothesis space favouring sequenced information,\nsuch as a series of chronological observations. Section X covers this\ntype of DNN, and the prominent sub-type of RNN: LSTMs.\n\nOne type of neural networks which is similar to MLP and popular in\ncontemporary research, is the CNN. These deep NN are essentially\nMLP which omit the property of being fully connected, in favour of\nrefining the hypothesis space towards detection of spatially-related features.","lastmodified":"2022-05-29T17:21:50.301778025Z","tags":null},"/notes/orbit-determination":{"title":"Orbit Determination and Parameter Estimation","content":"\nPredicting the state ($\\bm{x}_t$) of a spacecraft given an initial condition\n($\\bm{x}_0$), and models which form the equations of motion of the satellite,\n($\\dot{\\bm{x}}=f(t,\\bm{x})$), is a straightforward task involving the solution\nof an initial value problem (IVP) in the form of an ordinary differential\nequation (ODE). However the inverse problem  is more involved, that is, given a\nset of measurements ($\\bm{z}$) resulting from the dynamical system, we would\nlike to estimate the trajectory of the satellite and the parameters describing\nthe dynamical models, described mathematically as:\n\n$$\n\\begin{equation}\n    \\bm{x}(t) =\n    \\begin{bmatrix}\n        \\bm{r}(t) \\\\\n        \\bm{v}(t) \\\\\n        \\bm{p} \\\\\n        \\bm{q} \\\\\n    \\end{bmatrix},\n\\end{equation}\n$$\n\n$$\n\\begin{aligned}\n    \\textrm{where  }\n        \\bm{r}(t), \\bm{v}(t) \u0026= \\text{the position and velocity of the spacecraft as a function of time,} \\\\\n        \\bm{p}               \u0026= \\text{the parameters describing the force models,} \\\\\n        \\bm{q}               \u0026= \\text{the parameters describing the measurement models.} \\\\\n\\end{aligned}\n$$\n\nThe measurements made throughout the trajectory of the spacecraft at times\n$t_1,...,t_n$ are described by $\\bm{z}=[z_1,...,z_n]^T$, where each $z_i$ is\neither defined as a function of the state of the spacecraft at time $t_i$, or\nas a function of the state of the spacecraft at time $t_0$:\n\n$$\n\\begin{equation}\n    z_i(t_i) = g_i(t_i, \\bm{x}(t_i))+\\epsilon_i = h_i(t_i, \\bm{x}_0)+\\epsilon_i.\n\\end{equation}\n$$\n\n$$\n\\begin{aligned}\n    \\textrm{where  }\n        z_i \u0026= \\text{the i}^{th}\\text{ empirical measurement, assumed to be a random variable,} \\\\\n        g_i \u0026= \\text{the i}^{th}\\text{ model measurement as a function time and instantaneous state,} \\\\\n        h_i \u0026= \\text{the i}^{th}\\text{ model measurement as a function time and initial state,} \\\\\n        \\epsilon_i \u0026= \\text{the i}^{th}\\text{ residual, accounting for measurement errors.} \\\\\n\\end{aligned}\n$$\n\nThe expressions of $h_i$ and $g_i$ can be used interchangeably in the\nmeasurement model predictions, to account for the fact that the measurements are\noften made at different times than the respective instantaneous states of the\nspacecraft. This is done using variational equations, which are simulated to\nobtain the state transition matrix $\\bm{\\Phi}(t_0, t)$ of the spacecraft, which\nmay be interpolated for any arbitrary time within the temporal bounds of the ODE\nsolution across $t_i\\in[t_0, t_f]$, so that one may\nrelate an empirical $z_i$ at $\\bm{x}(t_i)$ to $\\bm{x}_0$ through $\\bm{\\Phi}(t_0,\nt_i)^{-1}\\bm{x}_0$. This effectively constrains the trajectory to the designed m\nIVP dynamical solution. Consequentially, the measurements concisely:\n\n$$\n\\begin{equation}\n    \\bm{z} = \\bm{h}(\\bm{x}_0) + \\bm{\\epsilon}.\n\\end{equation}\n$$\n","lastmodified":"2022-05-29T17:21:50.301778025Z","tags":null},"/notes/orbital-mechanics":{"title":"","content":"","lastmodified":"2022-05-29T17:21:50.301778025Z","tags":null}}