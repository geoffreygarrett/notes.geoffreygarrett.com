{"/":{"title":"Geoffrey's Notes","content":"\n## Vocational üíº\n\n- [Machine Learning MOC](notes/machine-learning.md)\n- [Estimation Theory MOC](notes/estimation-theory.md)\n- [Information Theory MOC](notes/information-theory.md)\n- [Software Engineering MOC](notes/software-engineering.md)\n\n## Avocational ‚öΩ\n\n## Useful üõ†Ô∏è\n\n- [All Notes](/notes)\n","lastmodified":"2022-05-30T19:12:58.893948289Z","tags":null},"/notes/deep-learning":{"title":"Deep Learning","content":"\nDeep Learning (DL) is a field of Machine Learning (ML) that is primarily\nconcerned with the learning of representations of data. At the core of DL is the\nuse of Multi-layer perceptrons (MLPs), used to model these representations. MLPs\nare fully connected layers of biologically inspired artifical neurons, also\nknown as perceptrons. A brief history are these biologically inspired models are\ncovered in Section X with adapted notation from the field of DL. Although not\nall practices in DL, strictly speaking, make use of MLPs, they are a fundamental\nconcept which must be understood in the stepping stones towards concepts of\nhigher complexity in the field. Section X covers this concept, extending\ndirectly on their composite component: perceptrons.\n\n**MLP** are also called **feedfoward** as information is propagated in\nonly a forward direction, as opposed to exhibiting **feedback** connections,\nwhere intermediate computations are fed back into the network. When feedforward\nnetworks are extended to include feedback connections, they are called\n**RNNs**. These types of networks excel at learning temporal\nfeatures, exhibiting a refined hypothesis space favouring sequenced information,\nsuch as a series of chronological observations. Section X covers this\ntype of DNN, and the prominent sub-type of RNN: LSTMs.\n\nOne type of neural networks which is similar to MLP and popular in\ncontemporary research, is the CNN. These deep NN are essentially\nMLP which omit the property of being fully connected, in favour of\nrefining the hypothesis space towards detection of spatially-related features.","lastmodified":"2022-05-30T19:12:58.893948289Z","tags":null},"/notes/estimation-theory":{"title":"Estimation Theory","content":"\n**Estimation theory** is a branch of statistics that addresses the estimation of\nunknown parameters basic on empirical measurements, which contain a random\ncomponent.\n\n- An **estimator** is an algorithm that attempts to approximate the unknown\n  parameters using measurements.\n\n## Estimators \n\n- [Least squares](notes/least-squares.md)\n\n## Applications\n\n- [Orbit determination](notes/orbit-determination.md)\n","lastmodified":"2022-05-30T19:12:58.893948289Z","tags":null},"/notes/information-theory":{"title":"Information Theory","content":"\n## Self-information (information content)\n\nThe self-information (a.k.a. _information content_, _surprisal_,\nor _Shannon information_) is a quantity used in information theory which\nis derived from the probability of a certain event occurring from a random\nvariable. The self-information was defined by Claude Shannon such that\nthe following axioms were met:\n\n- An event that is 100% probable is perfectly unsurprising and\n  therefore yields no information content.\n- The less probable an event is, the more surprising, and therefore the\n  more information it yields.\n- The total information of independently measured events, is the\n  sum of their respective self-information.\n\n$$\n\\begin{equation}\n    I(x):=-\\log_b(p(x))\n\\end{equation}\n$$\n\n\\subsection{Entropy (average information)}\n\n$$\n\\begin{equation}\n    H(x)=\\mathbb{E}[I(x)]\n\\end{equation}\n$$\n\n## Kullback-Leibler divergence (information gain)\n\nThe Kullback-Leibler divergence (a.k.a. _relative entropy_ and\n_I-divergence_) a measure of how one probability distribution $P$ differs\nfrom another, $Q$. For the distributions $P$ and $Q$ for a continuous random\nvariable, the relative entropy integral is:\n\n$$\n\\begin{equation}\n    D_{KL}(P\\;||\\;Q) = \\int_{-\\infty}^{\\infty}p(x)\\log(\\frac{p(x)}{q(x)})\\;dx.\n\\end{equation}\n$$\n\nNote that $D_{KL}(P\\;||\\;Q)$ is only finite if the support set of $P$ is\ncontained in the support set of $Q$. In the context of Bayesian inference\n$D_{KL}(P\\;||\\;Q)$, read as _the KL-divergence of P given Q_, is a\nmeasure of the information gained by revisiting one's beliefs from a prior\nprobability distribution $Q$ to a posterior probability distribution $P$. For\nexample the $D_{KL}(P\\;||\\;Q)$ for $P\\sim{}\\mathcal{N}(\\mu_1,\\sigma_1)$ and\n$Q\\sim{}\\mathcal{N}(\\mu_2,\\sigma_2)$ can be derived analytically to be:\n\n$$\n\\begin{equation}\n    D_{KL}(P\\;||\\;Q) = \\log\\frac{\\sigma_2}{\\sigma_1} + \\frac{\\sigma_1^2 + (\\mu_1-\\mu_2)^2}{2\\sigma_2^2} - \\frac{1}{2}.\n\\end{equation}\n$$\n\nOne noteworthy characteristic of the KL-divergence is its asymmetry, that is\n$D_{KL}(P\\;||\\;Q)\\neq{}D_{KL}(Q\\;||\\;P)$. This means that KL-divergence makes\nfor a poor \\textit{distance} metric as is commonly done with, for example,\nsquared-errors. This may at first present KL-divergence as a suboptimal choice\nas a general metric, however when considering the relation between the posterior\nand the priori, the relative information gain when travelling from one to the\nother **is** inherently asymmetric by their very nature. \n\n## Jensen-Shannon divergence (information radius)\n\nThe Jensen-Shannon divergence (a.k.a. _information radius_ and _total divergence\nto the average_) is based on the KL-divergence, however it has been extended\nwith the differences that it is symmetric, and always has a finite value.\n\n$$\n\\begin{equation}\n    D_{JS}(P\\;||\\;Q) = \\frac{1}{2}D_{KL}(P\\;||\\;M) + \\frac{1}{2}D_{KL}(Q\\;||\\;M)\n\\end{equation}\n$$\n$$\n\\text{where}\\;\\;\nM = \\frac{1}{2}(P+Q)\n$$","lastmodified":"2022-05-30T19:12:58.897948289Z","tags":null},"/notes/kalman-filter":{"title":"Kalman Filter","content":"","lastmodified":"2022-05-30T19:12:58.897948289Z","tags":null},"/notes/least-squares":{"title":"Least Squares","content":"\nThe least-squares method was officially discovered and published by Adrien-Marie\nLegendre in their work _\"Nouvelles methodes pour la determination des orbites\ndes cometes\"_, published in 1805 \\[citation needed\\].\n\n## Ordinary Least Squares\n\nOrdinary Least Squares (OLS)\n\n$$\n\\begin{equation}\n    {\\hat {\\beta }}={\\rm {arg}}\\min _{\\beta }\\,\\lVert z-X\\beta \\rVert,\n\\end{equation}\n$$\n\n- **Linearity in parameters**: The linear regression model is _linear in\n  parameters_, that is, $\\bm{z} = \\bm{A}\\bm{\\beta}+\\bm{\\epsilon}$.\n- **Strict exogenity**: The errors in the regression are should have conditional mean\n  zero, that is, $\\mathbb{E}[\\bm{\\epsilon}|\\bm{A}] = \\bm{0}$.\n\n{{\u003c svg src=\"/notes/images/OLS-geometric-interpretation.svg\" caption=\"Geometric interpretation of Ordinary Least Squares (OLS).\" \u003e}}\n\n## Weighted Least Squares\n\nThe Weighted Least Squares (WLS) method is an application of the\nGeneralised Least Squares (GLS) algorithm, which aims at\nestimating unknown parameters ($\\bm{\\beta}$) in a linear regression model, given a\nset of observations ($\\bm{z}$), where there is a certain degree of correlation\n($\\bm{W}$) between the residuals ($\\bm{\\epsilon}$) in the regression model. It\nis usually written as:\n\n$$\n\\begin{equation}\n    \\begin{aligned}\n        \\bm{z} \u0026= \\bm{A}\\bm{\\beta}+\\bm{\\epsilon}, \\\\\n        \\mathbb{E}[\\bm{\\epsilon}|\\bm{A}] \u0026= \\bm{0}, \\\\\n        \\text{Cov}(\\bm{\\epsilon}|\\bm{A}) \u0026= \\bm{W}. \\\\\n    \\end{aligned}\n\\end{equation}\n$$\n\nThe residual vector is defined as $\\rho=\\bm{z}-\\bm{A}\\bm{\\beta}$. The Weighted\nLeast Squares estimate ($\\bm{\\hat{\\beta}}$) is unbiased, consistent and\nefficient, and obtained through minimising $\\bm{\\rho}^T\\bm{W}^{-1}\\bm{\\rho}$. The\nestimate is given, without derivation, by:\n\n$$\n\\begin{equation}\n    \\begin{aligned}\n        \\bm{\\hat{\\beta}} \u0026= (\\bm{A}^T\\bm{W}^{-1}\\bm{A})^{-1}\\bm{A}^T\\bm{W}^{-1}\\bm{z}, \\\\\n        \\mathbb{E}[\\bm{\\hat{\\beta}}] \u0026= \\bm{\\beta}, \\\\\n        \\text{Cov}[\\bm{\\hat{\\beta}}|\\bm{A}] \u0026= (\\bm{A}^T\\bm{W}^{-1}\\bm{A})^{-1}. \\\\\n    \\end{aligned}\n\\end{equation}\n$$\n\n## Generalised Least Squares\n\nGLS was first described by Alexander Aitken in 1936. \\[[1](https://en.wikipedia.org/wiki/Generalized_least_squares#cite_note-1)\\]\n\n## Non-linear Least Squares\n\nThe problem arises when considering the highly non-linear modelled measurements\n$\\bm{h}(\\bm{x}_0)$ cannot be described by the linear relation above. For this\nreason, the model is linearized by approximation to a first-order Taylor\npolynomial expansion about $\\bm{x}_0^k$, where $k$ is an iteration number:\n\n$$\n\\begin{equation}\n    \\begin{aligned}\n        \\bm{z}                              \u0026=        \\bm{f}(\\bm{\\beta}) + \\bm{\\epsilon}                                                              \\\\\n        \\bm{f}(\\bm{\\beta}+\\Delta\\bm{\\beta}) \u0026\\approx  \\bm{f}(\\bm{\\beta}) + \\frac{\\partial{\\bm{f}(\\bm{\\beta})}}{\\partial{\\bm{\\beta}}}\\Delta\\bm{\\beta}  \\\\\n    \\end{aligned}\n\\end{equation}\n$$\n","lastmodified":"2022-05-30T19:12:58.897948289Z","tags":null},"/notes/machine-learning":{"title":"Machine Learning","content":"\n## Sub-fields\n- [Deep Learning](notes/deep-learning.md)","lastmodified":"2022-05-30T19:12:58.897948289Z","tags":null},"/notes/orbit-determination":{"title":"Orbit Determination and Parameter Estimation","content":"\nPredicting the state ($\\bm{x}_t$) of a spacecraft given an initial condition\n($\\bm{x}_0$), and models which form the equations of motion of the satellite,\n($\\dot{\\bm{x}}=f(t,\\bm{x})$), is a straightforward task involving the solution\nof an initial value problem (IVP) in the form of an ordinary differential\nequation (ODE). However the inverse problem  is more involved, that is, given a\nset of measurements ($\\bm{z}$) resulting from the dynamical system, we would\nlike to estimate the trajectory of the satellite and the parameters describing\nthe dynamical models, described mathematically as:\n\n$$\n\\begin{equation}\n    \\bm{x}(t) =\n    \\begin{bmatrix}\n        \\bm{r}(t) \\\\\n        \\bm{v}(t) \\\\\n        \\bm{p} \\\\\n        \\bm{q} \\\\\n    \\end{bmatrix},\n\\end{equation}\n$$\n\n$$\n\\begin{aligned}\n    \\textrm{where  }\n        \\bm{r}(t), \\bm{v}(t) \u0026= \\text{the position and velocity of the spacecraft as a function of time,} \\\\\n        \\bm{p}               \u0026= \\text{the parameters describing the force models,} \\\\\n        \\bm{q}               \u0026= \\text{the parameters describing the measurement models.} \\\\\n\\end{aligned}\n$$\n\nThe measurements made throughout the trajectory of the spacecraft at times\n$t_1,...,t_n$ are described by $\\bm{z}=[z_1,...,z_n]^T$, where each $z_i$ is\neither defined as a function of the state of the spacecraft at time $t_i$, or\nas a function of the state of the spacecraft at time $t_0$:\n\n$$\n\\begin{equation}\n    z_i(t_i) = g_i(t_i, \\bm{x}(t_i))+\\epsilon_i = h_i(t_i, \\bm{x}_0)+\\epsilon_i.\n\\end{equation}\n$$\n\n$$\n\\begin{aligned}\n    \\textrm{where  }\n        z_i \u0026= \\text{the i}^{th}\\text{ empirical measurement, assumed to be a random variable,} \\\\\n        g_i \u0026= \\text{the i}^{th}\\text{ model measurement as a function time and instantaneous state,} \\\\\n        h_i \u0026= \\text{the i}^{th}\\text{ model measurement as a function time and initial state,} \\\\\n        \\epsilon_i \u0026= \\text{the i}^{th}\\text{ residual, accounting for measurement errors.} \\\\\n\\end{aligned}\n$$\n\nThe expressions of $h_i$ and $g_i$ can be used interchangeably in the\nmeasurement model predictions, to account for the fact that the measurements are\noften made at different times than the respective instantaneous states of the\nspacecraft. This is done using variational equations, which are simulated to\nobtain the state transition matrix $\\bm{\\Phi}(t_0, t)$ of the spacecraft, which\nmay be interpolated for any arbitrary time within the temporal bounds of the ODE\nsolution across $t_i\\in[t_0, t_f]$, so that one may\nrelate an empirical $z_i$ at $\\bm{x}(t_i)$ to $\\bm{x}_0$ through $\\bm{\\Phi}(t_0,\nt_i)^{-1}\\bm{x}_0$. This effectively constrains the trajectory to the designed m\nIVP dynamical solution. Consequentially, the measurements concisely:\n\n$$\n\\begin{equation}\n    \\bm{z} = \\bm{h}(\\bm{x}_0) + \\bm{\\epsilon}.\n\\end{equation}\n$$\n","lastmodified":"2022-05-30T19:12:58.897948289Z","tags":null},"/notes/software-engineering":{"title":"Software Engineering","content":"\n- [SOLID Principles](notes/solid-principles.md)\n","lastmodified":"2022-05-30T19:12:58.897948289Z","tags":null},"/notes/solid-principles":{"title":"SOLID Principles","content":"\nIn software engineering, **SOLID** is a mnemonic acronym for five design\nprinciples intended to make object-oriented designs more **understandable**,\n**flexible**, and **maintainable**.\n\n## **S**ingle responsibility\n\n\u003e There should never be more than one reason for a class to change.\n\n## **O**pen‚Äìclosed\n\n\u003e Software entities ... should be open for extension, but closed for modification.\n\n## **L**iskov substitution\n\n\u003e Functions that use pointers or references to base classes must be able to use objects of derived classes without knowing it.\n\n## **I**nterface segregation\n\n\u003e Many client-specific interfaces are better than one general-purpose interface.\n\n## **D**ependency inversion\n\n\u003e Depend upon abstractions, \\[not\\] concretions.","lastmodified":"2022-05-30T19:12:58.897948289Z","tags":null}}