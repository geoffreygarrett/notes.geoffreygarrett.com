{"/":{"title":"Geoffrey's Notes","content":"\n- [Machine Learning MOC](notes/machine-learning.md)\n- [Estimation Theory MOC](notes/estimation-theory.md)\n\n- [All Notes](/notes)\n","lastmodified":"2022-05-29T16:01:57.33729232Z","tags":null},"/notes/estimation-theory":{"title":"Estimation Theory","content":"\n# Estimation Theory\n\n## Algorithms\n\n{{\u003c svg src=\"/notes/images/OLS_geometric_interpretation.svg\" \u003e}}\n\n{{\u003c figure src=\"/notes/images/OLS_geometric_interpretation.svg\" \u003e}}\n\n[comment]: \u003c\u003e ({{\u003csvg src=\"/notes/images/OLS_geometric_interpretation.svg\"\u003e}} )\n\n### Weighted Least-Squares Estimation\n\nThe Weighted Least-Squares Estimation method is an application of the\nGeneralized Least-Squares (GLS) algorithm, which aims at\nestimating unknown parameters ($\\bm{\\beta}$) in a linear regression model, given a\nset of observations ($\\bm{z}$), where there is a certain degree of correlation\n($\\bm{W}$) between the residuals ($\\bm{\\epsilon}$) in the regression model. It\nis usually written as:\n\n$$\n\\begin{equation}\n    \\begin{aligned}\n        \\bm{z} \u0026= \\bm{A}\\bm{\\beta}+\\bm{\\epsilon}, \\\\\n        \\mathbb{E}[\\bm{\\epsilon}|\\bm{A}] \u0026= \\bm{0}, \\\\\n        \\text{Cov}(\\bm{\\epsilon}|\\bm{A}) \u0026= \\bm{W}. \\\\\n    \\end{aligned}\n\\end{equation}\n$$\n\nThe residual vector is defined as $\\rho=\\bm{z}-\\bm{A}\\bm{\\beta}$. The Weighted\nLeast-Squares estimate ($\\bm{\\hat{\\beta}}$) is unbiased, consistent and\nefficient, and obtained through minimising $\\bm{\\rho}^T\\bm{W}^{-1}\\bm{\\rho}$. The\nestimate is given, without derivation, by:\n\n$$\n\\begin{equation}\n    \\begin{aligned}\n        \\bm{\\hat{\\beta}} \u0026= (\\bm{A}^T\\bm{W}^{-1}\\bm{A})^{-1}\\bm{A}^T\\bm{W}^{-1}\\bm{z}, \\\\\n        \\bm{\\hat{\\beta}} \u0026= \\text{argmin}(\\bm{\\rho}^T\\bm{W}^{-1}\\bm{\\rho}), \\\\\n        \\mathbb{E}[\\bm{\\hat{\\beta}}] \u0026= \\bm{\\beta}, \\\\\n        \\text{Cov}[\\bm{\\hat{\\beta}}|\\bm{A}] \u0026= (\\bm{A}^T\\bm{W}^{-1}\\bm{A})^{-1}. \\\\\n    \\end{aligned}\n\\end{equation}\n$$\n\n### Non-linear Weighted Least-Squares Estimation\n\nThe problem arises when considering the highly non-linear modelled measurements\n$\\bm{h}(\\bm{x}_0)$ cannot be described by the linear relation above. For this\nreason, the model is linearized by approximation to a first-order Taylor\npolynomial expansion about $\\bm{x}_0^k$, where $k$ is an iteration number:\n\n$$\n\\begin{equation}\n    \\begin{aligned}\n        \\bm{z}                              \u0026=        \\bm{f}(\\bm{\\beta}) + \\bm{\\epsilon}                                                              \\\\\n        \\bm{f}(\\bm{\\beta}+\\Delta\\bm{\\beta}) \u0026\\approx  \\bm{f}(\\bm{\\beta}) + \\frac{\\partial{\\bm{f}(\\bm{\\beta})}}{\\partial{\\bm{\\beta}}}\\Delta\\bm{\\beta}  \\\\\n    \\end{aligned}\n\\end{equation}\n$$\n\n## Applications\n\n### Orbit Determination and Parameter Estimation\n\nPredicting the state ($\\bm{x}_t$) of a spacecraft given an initial condition\n($\\bm{x}_0$), and models which form the equations of motion of the satellite,\n($\\dot{\\bm{x}}=f(t,\\bm{x})$), is a straightforward task involving the solution\nof an initial value problem (IVP) in the form of an ordinary differential\nequation (ODE). However the inverse problem  is more involved, that is, given a\nset of measurements ($\\bm{z}$) resulting from the dynamical system, we would\nlike to estimate the trajectory of the satellite and the parameters describing\nthe dynamical models, described mathematically as:\n\n$$\n\\begin{equation}\n    \\bm{x}(t) =\n    \\begin{bmatrix}\n        \\bm{r}(t) \\\\\n        \\bm{v}(t) \\\\\n        \\bm{p} \\\\\n        \\bm{q} \\\\\n    \\end{bmatrix},\n\\end{equation}\n$$\n\n$$\n\\begin{aligned}\n    \\textrm{where  }\n        \\bm{r}(t), \\bm{v}(t) \u0026= \\text{the position and velocity of the spacecraft as a function of time,} \\\\\n        \\bm{p}               \u0026= \\text{the parameters describing the force models,} \\\\\n        \\bm{q}               \u0026= \\text{the parameters describing the measurement models.} \\\\\n\\end{aligned}\n$$\n\nThe measurements made throughout the trajectory of the spacecraft at times\n$t_1,...,t_n$ are described by $\\bm{z}=[z_1,...,z_n]^T$, where each $z_i$ is\neither defined as a function of the state of the spacecraft at time $t_i$, or\nas a function of the state of the spacecraft at time $t_0$:\n\n$$\n\\begin{equation}\n    z_i(t_i) = g_i(t_i, \\bm{x}(t_i))+\\epsilon_i = h_i(t_i, \\bm{x}_0)+\\epsilon_i.\n\\end{equation}\n$$\n\n$$\n\\begin{aligned}\n    \\textrm{where  }\n        z_i \u0026= \\text{the i}^{th}\\text{ empirical measurement, assumed to be a random variable,} \\\\\n        g_i \u0026= \\text{the i}^{th}\\text{ model measurement as a function time and instantaneous state,} \\\\\n        h_i \u0026= \\text{the i}^{th}\\text{ model measurement as a function time and initial state,} \\\\\n        \\epsilon_i \u0026= \\text{the i}^{th}\\text{ residual, accounting for measurement errors.} \\\\\n\\end{aligned}\n$$\n\nThe expressions of $h_i$ and $g_i$ can be used interchangeably in the\nmeasurement model predictions, to account for the fact that the measurements are\noften made at different times than the respective instantaneous states of the\nspacecraft. This is done using variational equations, which are simulated to\nobtain the state transition matrix $\\bm{\\Phi}(t_0, t)$ of the spacecraft, which\nmay be interpolated for any arbitrary time within the temporal bounds of the ODE\nsolution across $t_i\\in[t_0, t_f]$, so that one may\nrelate an empirical $z_i$ at $\\bm{x}(t_i)$ to $\\bm{x}_0$ through $\\bm{\\Phi}(t_0,\nt_i)^{-1}\\bm{x}_0$. This effectively constrains the trajectory to the designed m\nIVP dynamical solution. Consequentially, the measurements concisely:\n\n$$\n\\begin{equation}\n    \\bm{z} = \\bm{h}(\\bm{x}_0) + \\bm{\\epsilon}.\n\\end{equation}\n$$\n","lastmodified":"2022-05-29T16:01:57.33729232Z","tags":null},"/notes/machine-learning":{"title":"Deep Learning Introduction","content":"\n## Deep Learning\nDL is a field of ML that is primarily concerned with the learning of\nrepresentations of data. At the core of DL is the use of MLP, used\nto model these representations. MLP are fully connected layers of\nbiologically inspired artifical neurons, also known as perceptrons. A\nbrief history are these biologically inspired models are covered in\nSection X with adapted notation from the field of DL.\nAlthough not all practices in DL, strictly speaking, make use of\nMLPs, they are a fundamental concept which must be understood in the\nstepping stones towards concepts of higher complexity in the field.\nSection X covers this concept, extending directly on their composite\ncomponent: perceptrons.\n\n**MLP** are also called **feedfoward** as information is propagated in\nonly a forward direction, as opposed to exhibiting **feedback** connections,\nwhere intermediate computations are fed back into the network. When feedforward\nnetworks are extended to include feedback connections, they are called\n**RNNs**. These types of networks excel at learning temporal\nfeatures, exhibiting a refined hypothesis space favouring sequenced information,\nsuch as a series of chronological observations. Section X covers this\ntype of DNN, and the prominent sub-type of RNN: LSTMs.\n\nOne type of neural networks which is similar to MLP and popular in\ncontemporary research, is the CNN. These deep NN are essentially\nMLP which omit the property of being fully connected, in favour of\nrefining the hypothesis space towards detection of spatially-related features.","lastmodified":"2022-05-29T16:01:57.33729232Z","tags":null},"/notes/orbital-mechanics":{"title":"","content":"","lastmodified":"2022-05-29T16:01:57.33729232Z","tags":null}}