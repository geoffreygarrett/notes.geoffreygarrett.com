{"/":{"title":"Geoffrey's Notes","content":"[Deep Learning MOC](notes/config.md)\n\n- [All Notes](/notes)","lastmodified":"2022-05-25T21:39:26.096217858Z","tags":null},"/notes/config":{"title":"Deep Learning Introduction","content":"## Deep Learning\nDL is a field of ML that is primarily concerned with the learning of\nrepresentations of data. At the core of DL is the use of MLP, used\nto model these representations. MLP are fully connected layers of\nbiologically inspired artifical neurons, also known as perceptrons. A\nbrief history are these biologically inspired models are covered in\nSection X with adapted notation from the field of DL.\nAlthough not all practices in DL, strictly speaking, make use of\nMLPs, they are a fundamental concept which must be understood in the\nstepping stones towards concepts of higher complexity in the field.\nSection X covers this concept, extending directly on their composite\ncomponent: perceptrons.\n\n**MLP** are also called **feedfoward** as information is propagated in\nonly a forward direction, as opposed to exhibiting **feedback** connections,\nwhere intermediate computations are fed back into the network. When feedforward\nnetworks are extended to include feedback connections, they are called\n**RNNs**. These types of networks excel at learning temporal\nfeatures, exhibiting a refined hypothesis space favouring sequenced information,\nsuch as a series of chronological observations. Section X covers this\ntype of DNN, and the prominent sub-type of RNN: LSTMs.\n\nOne type of neural networks which is similar to MLP and popular in\ncontemporary research, is the CNN. These deep NN are essentially\nMLP which omit the property of being fully connected, in favour of\nrefining the hypothesis space towards detection of spatially-related features.","lastmodified":"2022-05-25T21:39:26.096217858Z","tags":null}}